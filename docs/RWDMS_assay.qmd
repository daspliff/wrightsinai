---
title: "RWDMS Directory Structure Analysis"
author: Ryan Michael
date: 2023-09-01
output: 
  html_document:
    toc: true
    code_folding: show
---

## Introduction

In this document, we analyze the structure and contents of the **RWDMS** filesystem. We examine file sizes, file types, and the distribution of files across different directories. As part of this analysis, we also seek to identify any large files that are duplicated across multiple directories.

## Table of Contents
1. [Data Collection](#data-collection)
2. [Importing Data](#importing-data)
3. [Basic Structure Analysis](#basic-structure-analysis)
4. [File Type Analysis](#file-type-analysis)
5. [Checking for Duplicates](#checking-for-duplicates)
6. [File Inventory](#file-inventory)


## Data Collection
The data for this analysis was collected using a PowerShell script that recursively iterates through the RWDMS filesystem and outputs the directory structure and file information. The script (included below) generates a text file with the following columns:
*Directory, Full Path, File Name, File Extension, File Size, Last Write Time, and File Owner*.

```powershell
Get-ChildItem -Recurse -File | ForEach-Object { 
    $firstDir = ($_.DirectoryName -split '\\')[1] #
    $owner = (Get-Acl $_.FullName).Owner
    "$firstDir, $($_.DirectoryName), $($_.Name), $($_.Extension), $($_.Length), $($_.LastWriteTime), $owner"
} > D:\tree\file_paths_20230905.txt
```

# Importing Data

First, we import the data collected using the PowerShell script and sanitize the variable names. We'll print the first few rows of the data and the names of the directories to make sure everything looks good.

```{r, message=FALSE, warning=FALSE}
#| echo: true
#| code-fold: true
#| code-summary: "expand for full code"

library(tidyverse)
rdwms_data <- read.csv("/Users/RM/Library/CloudStorage/OneDrive-TheMountSinaiHospital/work/rwdms_assay/data/file_paths_20230905.txt",
    header = F,
    stringsAsFactors = FALSE,
    check.names = F,
    fileEncoding = "UCS-2LE") %>%
    rename(
        root_dir = V1,
        abs_path = V2,
        file_name = V3,
        file_ext = V4,
        file_size = V5,
        last_write = V6,
        file_owner = V7
        ) %>%
    mutate(
        file_owner = str_replace(file_owner, ".*\\\\", ""),
        last_write = format(as.POSIXct(last_write, format = "%Y-%m-%d %H:%M:%S"), "%Y-%m-%d")
        )


# Print first few rows of data
head(rdwms_data)
# Print the names of the directories
unique(rdwms_data$root_dir)
```

One obvious takeway from a cursory look at the raw dataset is the non-standardization of file names in the directories. For example, some directories use underscores to separate words, while others use spaces. Some directories use camel case, while others use all caps. Standardizing file names is an important practice in data science as it helps maintain consistency, organization, and ease of collaboration within a project or team.

# Basic Structure Analysis
Lets start by looking at the number of files in each directory. We'll use the `root_dir` column to group the data and then count the number of files in each group. Also, we'll remove the `tree` directory from the data since this was a directory we created to store the outputs generated from the powershell script. We'll then arrange the data in descending order by the number of files in each directory and calculate the total size of each directory. Finally, we'll add two columns that calculates the fractional number of files in each directory, and the fractional size of each directory, respectively.

```{r, message=FALSE, warning=FALSE}
#| echo: true
#| code-fold: true
#| code-summary: "expand for full code"

library(DT)
library(dplyr)
library(knitr)
library(scales)
library(kableExtra)

total_files <- rdwms_data %>%
    filter(root_dir != "tree") %>%
    nrow() # Total number of files

rdwms_data %>%
    filter(root_dir != "tree") %>%
    group_by(root_dir) %>%
    summarise(n_files = n(), total_size = sum(file_size)) %>%
    arrange(desc(n_files)) %>%
    mutate(
        n_files = n_files, # Remove comma formatting for calculation
        size_GB = total_size / (1024^3), # Convert bytes to GB
        frac_num = n_files / total_files, # Calculate percentages
        frac_size = total_size / sum(rdwms_data$file_size, na.rm = TRUE)
    ) %>%
    select(root_dir, n_files, size_GB, frac_num, frac_size) %>%
    mutate(
        n_files = scales::comma(n_files), # Add comma formatting
        size_GB = scales::comma(round(size_GB, 2)),
        frac_num = paste0(round(frac_num * 100, 2), "%"),
        frac_size = paste0(round(frac_size * 100, 2), "%")
    ) %>%
    datatable(
        options = list(
            columnDefs = list(list(className = "dt-left", targets = 1)),
            scrollX = TRUE, # Enable horizontal scrolling
            dom = "ft", # Add search box and table footer
            pageLength = 12, # Number of rows per page
            lengthMenu = c(10, 25, 50, 100), # Number of rows per page options
            ordering = TRUE # Enable column sorting
        ),
        caption = htmltools::tags$caption(
            style = "caption-side: bottom; text-align: center;",
            "Table 1: ", htmltools::em("Summary statistics of the RWDMS filesystem")
        )
    )%>%
    DT::formatStyle(columns = c(1, 2, 3, 4, 5), fontSize = "11pt")
``` 
<br>
As we can see, the `HAPs data` directory contains the most files (specifically, 98% of all files), followed by the `CTSA` directory. However, the `Environmental_Data` directory , which accounts for only *0.6%* of the number of files contains the most data (53%). We can assume that is because the `Environmental_Data` directory contains a a smaller number of relatively large files, while the `HAPs data` directory contains many smaller files. We'll explore this further in the next section.

Lets see what that looks like when we plot it. We'll create a bar plot of the number of files in each directory. Because the data is very skewed, as shown in the table above, we'll log-transform the y-axis and sort the directories in descending order of total number of files.

```{r, message=FALSE, warning=FALSE}
#| echo: true
#| code-fold: true
#| code-summary: "expand for full code"
#| label: fig-barplot1
#| fig-cap: "Number of files in each root directory (Y-Axis is log10 scale)"
#| fig-cap-align: "center"

rdwms_data %>%
    filter(root_dir != "tree") %>%
    group_by(root_dir) %>%
    summarize(Count = n()) %>%
    ggplot(aes(x = reorder(root_dir, -Count), y = Count)) +
    geom_bar(stat = "identity", fill = "grey55", alpha = 0.8) +
    theme_minimal() + # Set to minimal theme
    theme(
        axis.text.x = element_text(angle = 45, hjust = 1, size = 11),
        axis.text.y = element_text(size = 11),
        panel.background = element_blank(), # Remove panel background
        plot.background = element_blank()
    ) + # Remove plot background
    labs(
        title = "Number of Files in Each Root Directory",
        x = "Root Directory", y = "File Count (log10) "
    ) +
    scale_y_log10() # Add log-scale y-axis
```

This second plot displays both the number of files and the size of each directory (in GB), using a secondary y-axis.

```{r, message=FALSE, warning=FALSE} 
#| echo: true
#| code-fold: true
#| code-summary: "expand for full code"
#| label: fig-barplot2
#| fig-cap: "Number of files and total size of each root directory"
#| fig-cap-align: "center"

rdwms_data %>%
    filter(root_dir != "tree") %>%
    group_by(root_dir) %>%
    summarize(Count = n(), Size = (sum(file_size) / (1024^3))) %>%
    # filter(Size > 1.0) %>% # Filter out directories with less than 1GB of data
    ggplot(aes(x = reorder(root_dir, -Count))) +
    geom_bar(aes(y = Count),
        stat = "identity", width = 0.9,
        # position = position_dodge(width = 0.4), # Position bars side by side
        fill = "grey55", alpha = 0.8
    ) +
    geom_point(aes(y = Size * 500), shape = 15, size = 5.0, color = "red") +
    theme_minimal() +
    theme(
        plot.title = element_text(hjust = 0.5),
        plot.caption = element_text(hjust = 0),
        plot.background = element_rect(
            fill = "white", colour = "white",
            size = 0.5, linetype = "solid"
        ),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 11),
        axis.text.y = element_text(size = 11, color = "grey55"),
        axis.title.x = element_text(color = "black", size = 12),
        axis.title.y = element_text(color = "grey55", size = 12),
        axis.text.y.right = element_text(size = 11, color = "red"),
        axis.title.y.right = element_text(color = "red", size = 12),
        panel.background = element_blank()
    ) +
    labs(
        title = "Number of Files and Directory Size in Each Root Directory",
        x = "Root Directory"
    ) +
    scale_y_continuous(
        name = "Total number of files \n",
        labels = scales::comma,
        sec.axis = sec_axis(
            trans = ~ . / 500,
            name = "Directory Size (GB) \n",
            labels = scales::comma
        )
    )

```

# File Type Analysis
Lets take a look at the distribution of file types in the RWDMS filesystem. We'll start by tabulating the number of files of each file type. We'll then calculate the total space occupied by each file type by summing across the size variable for each file type. We'll also add a column that calculates the fractional number of files in each file type, and the fractional size of each file type, respectively. This summary table is displayed below.
 
```{r, message=FALSE, warning=FALSE}
#| echo: true
#| code-fold: true
#| code-summary: "expand for full code"

library(DT)
rdwms_data %>%
    group_by(file_ext) %>%
    summarize(count = n(), size_GB = (sum(file_size) / (1024^3))) %>%
    arrange(desc(count)) %>%
    mutate(
        num_frac = paste0(round(count / sum(count) * 100, 3), "%"),
        count = scales::comma(count),
        size_frac = paste0(round(size_GB / sum(size_GB) * 100, 3), "%"),
        size_GB = scales::comma(round(size_GB, 3))
    ) %>%
    datatable( # Use the datatable() function instead of kable() and kable_styling()
        options = list(
            scrollX = TRUE, # Enable horizontal scrolling
            columnDefs = list(list(className = "dt-left", targets = 1:5)),
            pageLength = 10, # Number of rows per page
            lengthMenu = c(10, 25, 50, 100), # Number of rows per page options
            ordering = TRUE # Enable column sorting
        ),
        caption = htmltools::tags$caption(
            style = "caption-side: bottom; text-align: center;",
            "Table 2: ", htmltools::em("Analysis of file types in the RWDMS filesystem")
        )
    ) %>%
    DT::formatStyle(columns = c(1, 2, 3, 4, 5), fontSize = "11pt")
``` 

# Checking for Duplicates
Lets check to see if there are any files that are duplicated across multiple directories. Because we are interested in the duplication of large files, we'll filter the dataset for files greater than 100MB. We'll start by creating a table of the number of files in each file name. We'll then arrange the data in descending order by the number of files in each file name and calculate the total size of each file name. We'll also add a column that calculates the fractional number of files in each file name, and the fractional size of each, respectively.

```{r, message=FALSE, warning=FALSE}
#| echo: true
#| code-fold: true
#| code-summary: "expand for full code"

# Filter for files with duplicate filenames and sizes
duplicate_files <- rdwms_data %>%
    filter(file_size >= 1024^2 * 100) %>% # Filter for files >= 100MB
    group_by(file_name, file_size) %>%
    filter(n() > 1) %>%
    arrange(file_name, file_size) %>%
    mutate(
        file_size = round(file_size / (1024^3), 2), # Convert bytes to GB
    )

duplicate_files %>%
    datatable( # Use the datatable() function instead of kable() and kable_styling()
        options = list(
            scrollX = TRUE, # Enable horizontal scrolling
            columnDefs = list(list(className = "dt-left", targets = 1:5)),
            pageLength = 10, # Number of rows per page
            lengthMenu = c(10, 25, 50, 100), # Number of rows per page options
            ordering = TRUE # Enable column sorting
        ),
        caption = htmltools::tags$caption(
            style = "caption-side: bottom; text-align: center;",
            "Table 3: ", htmltools::em("Listing duplicate files in the RWDMS filesystem (files >= 100MB)")
        )
    ) %>%
    DT::formatStyle(columns = c(1, 2, 3, 4, 5), fontSize = "11pt")
```

<br>

```{r, message=FALSE, warning=FALSE}
#| echo: true
#| code-fold: true
#| code-summary: "expand for full code"

# Count the number of duplicate files of each type
duplicate_counts <- duplicate_files %>%
    group_by(file_name, file_size, file_ext) %>%
    summarise(
        Count = n(),
        TotalSize = sum(file_size)
    )

# Calculate the total size of duplicate files
duplicate_files %>%
    group_by(file_name, file_size, file_ext) %>%
    summarise(
        Count = n(),
        TotalSize = sum(file_size)
    ) %>%
    datatable( # Use the datatable() function instead of kable() and kable_styling()
        options = list(
            scrollX = TRUE, # Enable horizontal scrolling
            columnDefs = list(list(className = "dt-left", targets = 1:5)),
            pageLength = 10, # Number of rows per page
            lengthMenu = c(10, 25, 50, 100), # Number of rows per page options
            ordering = TRUE # Enable column sorting
        ),
        caption = htmltools::tags$caption(
            style = "caption-side: bottom; text-align: center;",
            "Table 4: ", htmltools::em("Summary statistics of duplicate files in the RWDMS filesystem (files >= 100MB)")
        )
    ) %>%
    DT::formatStyle(columns = c(1, 2, 3, 4, 5), fontSize = "11pt")
```

<br> 

```{r, message=FALSE, warning=FALSE}
#| echo: true
#| code-fold: true
#| code-summary: "expand for full code"
#| label: fig-barplot3
#| fig-cap: "Summary statistics of RWDMS filesystem: Count and total size of duplicate files by file type (files >= 100MB)"
#| fig-cap-align: "center"

# Calculate the total number and size by file type
duplicate_files %>%
    group_by(file_ext) %>%
    summarise(
        TotalCount = n(),
        TotalSize = sum(file_size)
    ) %>%
    arrange(desc(TotalCount)) %>%
    # plot the data
    ggplot(aes(x = reorder(file_ext, -TotalCount), y = TotalCount)) +
    geom_bar(stat = "identity", fill = "grey55", alpha = 0.8) +
    geom_point(aes(y = TotalSize), shape = 15, size = 5.0, color = "red") +
    theme_minimal() +
    theme(
        plot.title = element_text(hjust = 0.5),
        plot.caption = element_text(hjust = 0),
        plot.background = element_rect(
            fill = "white", colour = "white",
            size = 0.5, linetype = "solid"
        ),
        axis.text.x = element_text(hjust = 1, size = 11),
        axis.text.y = element_text(size = 11, color = "grey55"),
        axis.title.x = element_text(color = "black", size = 12),
        axis.title.y = element_text(color = "grey55", size = 12),
        axis.text.y.right = element_text(size = 11, color = "red"),
        axis.title.y.right = element_text(color = "red", size = 12),
        panel.background = element_blank()
    ) +
    labs(
        x = "\n File Type"
    ) +
    scale_y_continuous(
        name = "Total number of large duplicate files \n",
        sec.axis = sec_axis(
            trans = ~.,
            name = "File Size (GB) \n"
        )
    )
```

# File Inventory
Lets create a searchble invetory of all the files in the RWDMS filesystem. We'll start by creating a table of all the files in the RWDMS filesystem. We are especially interested in environmental datsets across the filesystem, so we'll filter for files with text-based (`.csv`, `.txt`, `.json`), binary (`.nc`, `.ncf`, `.hdf`, `.h5`), geospatial (`.tif`, `.tiff`, `.shp`, `.shx`, `.dbf`, `.kml`, `.kmz`, `.grib`, `.las`), database (`.sql`, `.sqlite`, `.db`, `.gdb`), and archive (`.zip`, `.rds`, `RData`) file extensions.

```{r, message=FALSE, warning=FALSE}
#| echo: true
#| code-fold: true
#| code-summary: "expand for full code"

# Create a table of all files in the RWDMS filesystem
rdwms_data %>%
    filter(root_dir != "tree") %>%
    filter(
        tolower(trimws(file_ext)) %in% c(
            ".csv", ".txt", ".json",
            ".nc", ".ncf", ".hdf", ".h5",
            ".tif", ".tiff", ".shp",
            ".kml", ".kmz", ".grib", ".las",
            ".sql", ".sqlite", ".db", ".gdb"
        )
    ) %>%
    filter(file_size >= 1024^2 * 100) %>% # Filter for files >= 100MB
    arrange(file_ext) %>%
    mutate(
        file_size = round(file_size / (1024^3), 2), # Convert bytes to GB
    ) %>%
    datatable( # Use the datatable() function instead of kable() and kable_styling()
        options = list(
            scrollX = TRUE, # Enable horizontal scrolling
            columnDefs = list(list(className = "dt-left", targets = 1:5)),
            pageLength = 10, # Number of rows per page
            lengthMenu = c(10, 25, 50, 100), # Number of rows per page options
            ordering = TRUE # Enable column sorting
        ),
        caption = htmltools::tags$caption(
            style = "caption-side: bottom; text-align: center;",
            "Table 5: ", htmltools::em("Inventory of large environmental datasets in the RWDMS filesystem (files >= 100MB)")
        )
    ) %>%
    DT::formatStyle(columns = c(1:7), fontSize = "11pt")
```

